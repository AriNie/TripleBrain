---
layout: post
title: 自然言語処理リサーチ１ # ページタイトル
tags: nlp BERT Transformer T5 DNN machine-learning # タグ
categories: machine-learning # カテゴリ. AtCoderの解説記事は atcoder で.
---


* TOC
{:toc}

Author: Non　<!-- 自分の名前 -->

<!-- ↓↓↓↓↓ 記事内容 ↓↓↓↓↓ -->

<br><br>

---

# 目次

自然言語処理の基本知識
- [コーパスとは](#コーパスとは)
- [テキスト前処理](#テキスト前処理)

最新の自然言語処理
- [Transformer](#Transformer)

<br><br>

---

# コーパスとは

<br>

コーパスとは自然言語で記述あるいは口述され、コンピュータ上に格納されたデータのこと。
自然言語データセットをコーパスと呼ぶという認識で大丈夫な気がする。

最近は多くのコーパスがWeb上に公開されている。テキスト分類コーパスには20 Newsgroups, IMDb, CoNll-2003, SQuAD, MS MARCOなどがある。

また、コーパスは**ラベル付きコーパス**と**ラベルなしコーパス**に二分される。

<br>

### ラベル付きコーパス
ラベル付きコーパスはテキストに対して教師データが付いているコーパスのこと。
品詞などにアノテーションされている。

<br>

### ラベルなしコーパス
ラベルなしコーパスとは教師なしのコーパス。

<br>

## コーパスの読み込み
コーパスの読み込みには
* CSV
* TSV
* JSON

などがある。

CSV, TSVにはpandasでの操作が適用されることが多い。

対してJSONではPandasの他にParseできるものが多くあり、自前で実装しやすい。またBS4などを使用してスクレイピングする場合はJSONが多い。

<br>

## アノテーション方法
アノテーションはアノテーションツールというのが用意されていて、それを使用することが推奨されている。

主なアノテーションツールは以下の通りである。

| | brat | doccano | prodigy | tagtog | LightTag |
|:---:|:---:|:---:|:---:|:---:|:---:|
|テキスト分類|✖︎|○|△|○|○| 
|系列ラベリング|○|○|○|○|○|
|系列変換|✖︎|○|✖︎|✖︎|✖︎|
|関係|○|✖︎|△|○|✖︎|

<br>

# テキスト前処理

テキストの前処理には以下のようなものなどがある。
* テキストクリーニング
* 単語分割
* 単語の正規化
* スワップワードの除去
* 単語のID化
* パディング

<br>

## テキストのクリーニング

テキストのクリーニングとはHTMLタグの除去のことである。例えば

```
この記事をご覧になっている方は
<span color="red">Word2vec</span>
についてご存知かもしれません。
```

という物があったときに以下のようにテキストクリーニングをする。

```
この記事をご覧になっている方は
Word2vec
についてご存知かもしれません。
```

このようなテキストクリーニングをPythonで行う際は、Bearutiful Soupやlxmlのようなライブラリが推奨される。
その際、正規表現を使用する場面が多く、正規表現の知識が必要となる。

<br>

## 単語分割

単語分割とはテキストを単語に分割する処理である。日本語では主に**形態素解析器**を用いる。

例えば、`彼女と国立新美術館へ行った。`とあった場合、これを

`彼女`, `と`, `国立`, `新`, `美術館`, `へ`, `行っ`, `た`, `。`　とする。

|彼女|と|国立|新|美術館|へ|行っ|た|。|
|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|
|名詞|助詞|名詞|接頭詞|名詞|助詞|動詞|助動詞|記号|

<br>

原形にした場合

|彼女|と|国立|新|美術館|へ|行く|た|。|
|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|
|名詞|助詞|名詞|接頭詞|名詞|助詞|動詞|助動詞|記号|

上の場合、`新国立美術館`が`新`, `国立`, `美術館`と３つに分解されている。これは解析に使っている辞書に`新国立美術館`という単語が含まれていないからである。

このように新語に強くないというのが単語分割の問題点である。

有名な辞書ではNEologdというものがある。

<br>

## 単語の正規化

単語の正規化では単語の文字種の統一化を行う。例えば、`ﾈｺ`という半角文字は全角文字`ネコ`という文字と同一のものとする。

```
iPhoneだって半角ｶﾅを打てる。
```

という文章は

```
iphoneだって半角カナを打てる。
```

という文字に変換する。

また、数字や辞書を用いた変換も行う。

```
ソニーの56000円のエクスペリア
```

変換後
```
Sonyの0円のXperia
```

＊辞書などをエスケープした数字は全て0に置き換えられる。

<br>

## ストップワードの除去

ストップワードというのは一般的で役に立たない等の理由で処理対象外とする単語のこと。
例えば、「は」「の」「です」「ます」など。

これらの単語は修験頻度が高いが役に立たないので計算量や性能に悪影響を及ぼすので除外する。

<br>

ストップワードの除去には以下の２つの方式などがある。
* 辞書による方式
* 出現頻度による方式

前者は除外するストップワードを辞書として持っておくことで処理を可能にする。

後者は単語出現頻度が高いものを除外していく。

<br>

## 単語のID化

単語をIDに置き換える処理。単語をIDにエンコードしていく。

<br>

## パディング

パディングというは入力にデータを加えて系列領を合わせる処理のこと。

要領は画像におけるNNでのパディングと同じ。

自然言語処理においては各レイヤーの入力のサイズを揃えるために行う。

たとえば

```
[[1 2 3]
 [1 2 3 4 5 6]
 [1 2 3 4]]
 ```

 というエンコードされた文章データがあったとき、以下のようにパディングしサイズを揃える。

 ```
 [[1 2 3 0 0 0 0 0 0 0]
 [1 2 3 4 5 6 0 0 0 0]
 [1 2 3 4 0 0 0 0 0 0]]
 ```

<br><br>

---

# Transformer

NLP（自然言語処理）の業界では、2018年10月にBERTというモデルが発表されたときにTransformerが注目されるようになった。

<br>

この記事を書いている時点ではT5（Text-To-Text Transfer Transformer）が最強のモデルとして君臨している。その以前にも2019年にBERTをこえるモデルとして注目を浴びたXLNetというモデルやGPT-2などが発表されてきた。

これらのモデルのベースとなっているのがTransformerである。

<br>

## Transformer出現までのNLP

Transformerが発表されるまでNLPの一般的な手法（特に翻訳のようなInput、Outputともに文章）というのは**エンコーダ-デコーダモデル**であった。

以下に例としてエンコーダ-デコーダモデルによる機械翻訳を見てみる。

<br>

### エンコーダ-デコーダモデルの機械翻訳

エンコーダ-デコーダモデルの機械翻訳はSeq-to-Seq翻訳（Sequence-to-sequence、後述）にニューラルネットワークを使用したものである。具体的にはSeq-to-Seq翻訳の中でも最も単純なモデルで、RNN（リカレントNN）を用いている。

<br>

実際には原言語側と目的言語側で2本のRNNを結合する
* 原言語側（Encoder）：入力単語の情報を蓄積するRNN
* 目的言語側（Decoder）：蓄積された情報を取り出しながら単語を生成するRNN

そして、エンコーダとデコーダの間で橋渡ししているのがVAEにおける潜在変数ベクトルのようなベクトル情報である。

<br>

![encdec1][encdec1]

> <a href="https://www.slideshare.net/YusukeOda1/encoderdecoder-tis" target="_blank">Encoder-decoder 翻訳 (TISハンズオン資料) Yusuke Oda</a>

<br>

入力側にある層は特定の単語の意味をベクトルを表現する埋め込み層（Embedding layer）である。

その内側にあるのが中間層が単語の意味の蓄積・放出する隠れ層である。

そして、出力側にあるのが次に生成する単語の確率を推定するSoftmax関数を活性化関数とする出力層である。

<br>

#### Seq2SeqやRNNの課題

* RNNは長期記憶が難しい（LSTMでも根本的解決にはならない）。つまり最初の方の文章の記憶とあとの文章の記憶が同等に扱われない。
* エンコーダ側とデコーダ側を結ぶベクトルの情報が小さくなりがち。（大きくすると短文での処理がうまく行かないというトレードオフの関係のため）
* （特にバニラなRNNでは）翻訳タスクなどで文字の長さがInputとOutputで同じになってしまう。（一般的に翻訳タスクなどではInputとOutputの長さは同じになるとは限らない。）＊
* 文字数が多い場合、勾配消失問題や勾配がエクスプロアしてしまう可能性があり、学習が困難になる。＊

＊：この文字数に関する問題への対処としてGRUやLSTMのエンコーダ-デコーダモデルが長く用いられ、ハイパボリックタンジェントはReLUに置き換えられてきた。
    GRUなどでは単語の長期記憶を可能にし、N input M outputを可能にしたが、エンコーダからデコーダに渡すコンテキストベクトルだけでは情報的に十分ではないという問題が発生していた。



<br>

### （おまけ）機械翻訳の体系的なイメージ

機械翻訳において使用する情報やアルゴリズムによって手法は様々で、分類すると以下のようになる。
* ルールベース翻訳：人間が書いた変換ルールを使用して翻訳
* 用例翻訳：登録された文の単語を置き換えて翻訳
* **統計的機械翻訳**（Statical Machine Translation, SMT）：大量の対の訳文から必要な情報を自動的に学習する。

<br>

#### 統計的機械翻訳
* フレーズ翻訳：原言語の単語列を使用　←**直訳**
* 構文翻訳：原言語の構文木を使用　←**直訳**
* Seq-to-seq翻訳：原言語の意味表現を使用　←**意訳**

<br>

### Attention

Attentionとは人間の目の仕組みに着目してSeq2Seqの問題点を解決するために考えられたNNの構造。

例えば、このような画像があったとき
![attention-img1](https://lilianweng.github.io/lil-log/assets/images/shiba-example-attention.png)
> <a href="https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html" target="_blank">Attention?Attention!</a>

人は耳の部分に焦点を合わせながら低解像度で周辺のパーツも見ている。

このような周辺の情報を低解像度で得ているというアイデアをNNに取り入れたものがAttentionである。

<br>

実際は以下の画像のようにRNNの各隠れ層からDecoder側のRNNのContextベクトルへ全結合層でショートカットさせる。
![attention-img2](https://cdn-ak.f.st-hatena.com/images/fotolife/a/aru47/20200818/20200818174132.png)
> <a href="" target="_blank">RNNからTransformerまでの歴史を辿る ～DNNを使ったNLPを浅く広く勉強～</a>

もう少し詳しく見てみると、このようになる。

![attention-img3](https://miro.medium.com/max/666/0*VrRTrruwf2BtW4t5.)
> <a href="https://medium.com/syncedreview/a-brief-overview-of-attention-mechanism-13c578ba9129" target="_blank">A Brief Overview of Attention Mechanism</a>

上の図では、青色の部分がエンコーダで赤色の部分がデコーダである。

コンテキストベクトルに注目すると、コンテキストベクトルはOutputのセルすべてをコンテキストベクトルに対してのInputとして持っているということがわかる。これはデコーダが生成するそれぞれのシングルワードに対してのソースワード（図のような翻訳タスクではTranslateする原言語）の確率分布を計算するために行っている。

このメカニズムを使用することによって、デコーダは一つの隠れた状態のみに基づいた推測ではなく、文章全体の大局的な情報を得ることができるようになっている。

これによってデコーダ側のContextベクトルはエンコーダ側の入力に依存した行列情報とすることができる。

ここで

- デコーディング中、Context-VectorはすべてのOutputワードに対して計算される2次元の行列（`Inputの次元 * Outputの次元`）である。
- 一度Context-Vectorが計算されるとAttention-VectorはAttention-Function `f`を用いて、`f(Context-Vector, ターゲットワード)`で計算される。

ということに注意されたい。また、このAttention機構の重み付けではLuongによる計算方法とBahdanauによる計算方法が存在する。

Transformerが出る直前では上で述べたAttentionを用いたエンコーダ-デコーダ型のRNNやCNNが主流となった。

<br>

### Attentionの一般化

Attentionを一般化すると「SourceとTargetという情報からAttentionスコアを計算する」という事になる。

上で述べたようなNN＋Attentionの構造のものは
* Source：Encoder側のNN隠れ層
* Target：Decoder側のNN隠れ層

ということになる。

<br>

NNを用いないTransformer（Self-Attention構造、後述）などでは、このSourceを検索クエリ（query）、Targetをkeyとvalueに分割する。この時、TargetとなるkeyとqueryはMemoryとしての役割があることに注意されたい。

ここでMemoryをkeyとvalueの二つに分割する意義についてだが、その意義は**知識ベースや文献などの記憶メモリに対してkeyとvalueという非自明な変換を行い、高い表現を実現するため**とされている。

<br>

## Transformerの特徴

Transformerの特徴は以下の通りである。
* self-Attention：RNNもCNNも用いず、Attentionのみを用いる。
* 少ない計算量：並列化がかなりしやすく、訓練時間がかなり削減できる。
* 汎用性：あらゆるタスクに対応でき、汎用性がある。

<br>

## Transformerの計算量

RNNの代わりにCNNを用いることで並列処理が可能になったもののせいぜい`O(log N)`になった程度であった。

しかし、Transformerでは文章のながさによらない`O(1)`（定数オーダー）とすることを可能にした。

<br>

## self-Attentionレイヤー

基本的にはAttentionと数学的アプローチやコンセプトは同じ。Self-attentionのメカニズムはInput同士をやりとりさせ、どのInputが注意を払うべきかを見つける注意機構である。

Self-attentionの流れ
1. Inputの準備
2. 重みの初期化
3. keyとquery、valueの準備
4. Input1からAttentioinスコアを算出
5. softmaxに通す
6. それぞれのInputに対応するValueにスコアを掛け合わせる
7. それぞれの6の結果を足し合わせてOutput1とする
8. Input2、3に対しても4から7を行う

＊上で行われる行列積のプロセスは**各エレメントの類似度を算出する**意味合いがあることに注意

### Step 1: Inputの準備
今回はInputとして以下のようなものを用意する。

![self-attetion-img1](https://miro.medium.com/max/700/1*hmvdDXrxhJsGhOQClQdkBA.png)
> <a href="https://towardsdatascience.com/illustrated-self-attention-2d627e33b20a" target="_blank">Illustrated: Self-Attention</a>

```
Input 1: [1, 0, 1, 0]
Input 2: [0, 2, 0, 2]
Input 3: [1, 1, 1, 1]
```

### Step 2: 重みの初期化

KeyやValue、Queryを得るために、それぞれの重みの初期化を行う。通常、NNでの重み初期化と同様にしてガウス分布などから乱数を生成し、それを初期の重みとする。

![self-attetion-img2](https://miro.medium.com/max/700/1*VPvXYMGjv0kRuoYqgFvCag.gif)
> <a href="https://towardsdatascience.com/illustrated-self-attention-2d627e33b20a" target="_blank">Illustrated: Self-Attention</a>

keyに対する初期重み
```
[[0, 0, 1],
 [1, 1, 0],
 [0, 1, 0],
 [1, 1, 0]]
```

queryに対する初期重み
```
[[1, 0, 1],
 [1, 0, 0],
 [0, 0, 1],
 [0, 1, 1]]
```

valueに対する初期重み
```
[[0, 2, 0],
 [0, 3, 0],
 [1, 0, 3],
 [1, 1, 0]]
```

### Step 3: keyとquery、valueの準備

それぞれのInputに対してkey, query, valueの重みを掛けていってkey, query, valueの計算をする。

Input1のKey
```
               [0, 0, 1]
[1, 0, 1, 0] x [1, 1, 0] = [0, 1, 1]
               [0, 1, 0]
               [1, 1, 0]
```

Input2のKey
```
               [0, 0, 1]
[0, 2, 0, 2] x [1, 1, 0] = [4, 4, 0]
               [0, 1, 0]
               [1, 1, 0]
```

Input3のKey
```
               [0, 0, 1]
[1, 1, 1, 1] x [1, 1, 0] = [2, 3, 1]
               [0, 1, 0]
               [1, 1, 0]
```

これらのKeyの計算はまとめると以下のようにできる。

```
               [0, 0, 1]
[1, 0, 1, 0]   [1, 1, 0]   [0, 1, 1]
[0, 2, 0, 2] x [0, 1, 0] = [4, 4, 0]
[1, 1, 1, 1]   [1, 1, 0]   [2, 3, 1]
```

![self-attetion-img3](https://miro.medium.com/max/700/1*dr6NIaTfTxEWzxB2rc0JWg.gif)
> <a href="https://towardsdatascience.com/illustrated-self-attention-2d627e33b20a" target="_blank">Illustrated: Self-Attention</a>

同様にしてまとめてQueryとValueの計算をしていく。

それぞれのInputに対してのQueryの計算
```
               [1, 0, 1]
[1, 0, 1, 0]   [1, 0, 0]   [1, 0, 2]
[0, 2, 0, 2] x [0, 0, 1] = [2, 2, 2]
[1, 1, 1, 1]   [0, 1, 1]   [2, 1, 3]
```

![self-attetion-img4](https://miro.medium.com/max/700/1*wO_UqfkWkv3WmGQVHvrMJw.gif)
> <a href="https://towardsdatascience.com/illustrated-self-attention-2d627e33b20a" target="_blank">Illustrated: Self-Attention</a>

それぞれのInputに対してのValueの計算
```
               [0, 2, 0]
[1, 0, 1, 0]   [0, 3, 0]   [1, 2, 3] 
[0, 2, 0, 2] x [1, 0, 3] = [2, 8, 0]
[1, 1, 1, 1]   [1, 1, 0]   [2, 6, 3]
```

![self-attetion-img5](https://miro.medium.com/max/700/1*5kqW7yEwvcC0tjDOW3Ia-A.gif)
> <a href="https://towardsdatascience.com/illustrated-self-attention-2d627e33b20a" target="_blank">Illustrated: Self-Attention</a>

### Step 4: Input1からAttentioinスコアを算出

Input1のAttentionスコアを得るためにInput1のQueryとそれぞれのInput（Input1自身も含む）のKeyとの行列積をとる。（Input2, 3のときも同様にして行う。）

![self-attetion-img6](https://miro.medium.com/max/700/1*u27nhUppoWYIGkRDmYFN2A.gif)
> <a href="https://towardsdatascience.com/illustrated-self-attention-2d627e33b20a" target="_blank">Illustrated: Self-Attention</a>

```
            [0, 4, 2]
[1, 0, 2] x [1, 4, 3] = [2, 4, 4]
            [1, 0, 1]
```

＊今回はスケール化内積注意機構と呼ばれる自己注意機構であるが、そのほかにも何個かの注意機構（加法注意機構など）が存在し、改良手法も提案されている。
> <a href="https://xtech.nikkei.com/atcl/nxt/mag/rob/18/00007/00006/" target="_blank">《日経Robo》自己注意機構：Self-Attention、画像生成や機械翻訳など多くの問題で最高精度</a>

### Step 5: softmaxに通す

4で算出した結果をSoftmax関数に通して確率としておく。

![self-attetion-img7](https://miro.medium.com/max/700/1*jf__2D8RNCzefwS0TP1Kyg.gif)
> <a href="https://towardsdatascience.com/illustrated-self-attention-2d627e33b20a" target="_blank">Illustrated: Self-Attention</a>

Input1に対しての場合
```
softmax([2, 4, 4]) = [0.0, 0.5, 0.5]
```

### Step 6: それぞれのInputに対応するValueにスコアを掛け合わせる

それぞれのInputに対応するValueに対して5でSoftmaxに通したAttentionスコアを掛け合わせる。

![self-attetion-img8](https://miro.medium.com/max/700/1*9cTaJGgXPbiJ4AOCc6QHyA.gif)
> <a href="https://towardsdatascience.com/illustrated-self-attention-2d627e33b20a" target="_blank">Illustrated: Self-Attention</a>

```
1: 0.0 * [1, 2, 3] = [0.0, 0.0, 0.0]
2: 0.5 * [2, 8, 0] = [1.0, 4.0, 0.0]
3: 0.5 * [2, 6, 3] = [1.0, 3.0, 1.5]
```

### Step 7: それぞれの6の結果を足し合わせてOutput1とする

6で計算した結果を全て足し合わせて、その結果をOutput1とする。

![self-attetion-img9](https://miro.medium.com/max/700/1*1je5TwhVAwwnIeDFvww3ew.gif)
> <a href="https://towardsdatascience.com/illustrated-self-attention-2d627e33b20a" target="_blank">Illustrated: Self-Attention</a>

```
  [0.0, 0.0, 0.0]
+ [1.0, 4.0, 0.0]
+ [1.0, 3.0, 1.5]
-----------------
= [2.0, 7.0, 1.5]
```

### Step 8: Input2、3に対しても4から7を行う

Input2, Input3についてもStep4~7を行う。

![self-attetion-img10](https://miro.medium.com/max/700/1*G8thyDVqeD8WHim_QzjvFg.gif)
> <a href="https://towardsdatascience.com/illustrated-self-attention-2d627e33b20a" target="_blank">Illustrated: Self-Attention</a>

<br>


### Attentionまとめ

ここでAttetionをまとめる。

![attetion-sum-img1](https://cdn-ak.f.st-hatena.com/images/fotolife/R/Ryobot/20171221/20171221163903.png)

> <a href="https://deeplearning.hatenablog.com/entry/transformer" target="_blank">論文解説 Attention Is All You Need (Transformer)</a>

このとき、keyとValueは一対一対応することから、**key-valueペアの辞書オブジェクト**として考えることができることに注意されたい。

![attention-sum-img2](https://cdn-ak.f.st-hatena.com/images/fotolife/R/Ryobot/20171221/20171221163943.png)

> <a href="https://deeplearning.hatenablog.com/entry/transformer" target="_blank">論文解説 Attention Is All You Need (Transformer)</a>

つまり、Attentionは検索クエリ（query）に一致するkeyをを索引して対応するvalueを取り出す操作であるという理解もできる。その理解の上で考えると、上で述べた「**keyとvalue間の肘名な返還によって高い表現力が得られる**」というkeyとvalueを分離する意義がより明確になる。

## 複数ヘッドAttention

上で説明したスケール化内積注意機構（シングルヘッドのスケール化内積注意機構）は別名縮小付き内積Attentionと呼ばれ、各単語に対して１組のQuery、Key、Valueで構成されている。しかし、１つの単語に対して複数のQuery、Key、Valueを持たせてやって考慮できる潜在変数が多した方がいいという考えはごく当たり前に出てくる。そこで登場するのが複数ヘッドAttention機構である。

<br>

具体的には
1. 縮小付き内積Attentionと同様にして各Query、Key、Valueに対してHeadの潜在表現を算出する
2. それぞれのHeadの潜在表現をConcatし、元の次元サイズに戻す

<br>

## Transformer全体像

Transformerの全体像は以下のようになっている。左側部分がEncoder、右側部分がDecoderになっている。

![transformer-view-img](https://qiita-user-contents.imgix.net/https%3A%2F%2Fimgur.com%2FBL9BicN.png?ixlib=rb-1.2.2&auto=format&gif-q=60&q=75&s=e31b67e650a37874b90fa8e9d03b6c6b)

> <a href="https://qiita.com/omiita/items/07e69aef6c156d23c538#%E3%83%87%E3%82%A3%E3%83%BC%E3%83%97%E3%83%A9%E3%83%BC%E3%83%8B%E3%83%B3%E3%82%B0%E7%95%8C%E3%81%AE%E5%A4%A7%E5%89%8D%E6%8F%90transformer%E3%81%AE%E8%AB%96%E6%96%87%E8%A7%A3%E8%AA%AC" target="_blank">深層学習界の大前提Transformerの論文解説！</a>

<br>

この`N`というのは6層になっていて各層複数ヘッドAttention機構になっている。

また、Encoder,Decoder共に複数のAttentionが用いられている。
* エンコーダ-デコーダ間のAttention：デコーダ側前層の出力部分にQuery＋エンコーダ側最終出力にKey, Querry
* エンコーダ側内部のSelf-Attention
* デコーダ側内部のMasked Self-Attention

<br>

また、Feed Forwardとなっている部分はサブ層で単に各単語ごとに独立した全結合層NNが存在している。（各NN内では他の単語との干渉はなく、同じ単語間では重みを共有している。）

<br>

## このセクションで今後書き足すべきこと
* self-attentionの理論部分
* 位置エンコーディング
* ~~multi-head Attention機構~~
* 自己回帰を使わないTransformer
* （DeepL翻訳などの最近の翻訳タスクのスコア状況）

<br>

> <a href="https://deeplearning.hatenablog.com/entry/transformer" target="_blank">論文解説 Attention Is All You Need (Transformer)</a>

> <a href="https://qiita.com/omiita/items/07e69aef6c156d23c538#%E3%83%87%E3%82%A3%E3%83%BC%E3%83%97%E3%83%A9%E3%83%BC%E3%83%8B%E3%83%B3%E3%82%B0%E7%95%8C%E3%81%AE%E5%A4%A7%E5%89%8D%E6%8F%90transformer%E3%81%AE%E8%AB%96%E6%96%87%E8%A7%A3%E8%AA%AC" target="_blank">深層学習界の大前提Transformerの論文解説！</a>

<br><br>

---

# BERT

BERT(Bidirectional Encoder Representations from Transformers)は、**TransformerのEncoderを使ったモデル**である。

そして、このTransfomerのEncoderというアーキテクチャーはどのようなタスクに対しても統一されている。

<br>

BERTの構成

| | Transformerのブロック数 | 隠れ層のサイズ | self-attentionのヘッド数 |パラメーター数|
|:---:|:---:|:---:|:---:|:---:|:---:|
|BERT-base|12|768|12|1.1億|
|BERT-large|24|1024|16|3.4億|

<br>

また、その特徴として事前学習でMLM(Masked Language Modeling)とNSP(Next Sentence Prediction)を学習させて精度向上が行われている。

<br>

一般的にnlpモデルに対して行われる事前学習には
* 特徴量ベース：事前学習で得られた表現ベクトルを特徴量のひとつとして用いるもの
* ファインチューニングを別途行う系：事前学習によって得られたパラメータを重みの初期値として学習させるもの

の二つがある。

<br>

そしてBERTで行われているには
* MLM：複数箇所が穴抜きになっている文章の単語予測（トークン予測）
* NSP：２分が渡されて、連続した文章かどうか判定する

というタスクが設けられ、事前学習している。

<br>

つまり、BERTでは
1. MLM、NSPによる事前学習
2. 事前学習の重みを初期値としたラベルありデータでのファインチューニング

という流れで学習させていく。

<br>

## MLM事前学習
入力の15%のトークンをマスクし、元のトークンを当てる。

<br><br>

---

# T5

T5(Text-To-Text-Transfer-Transformer)は分類、翻訳などのタスクをText-to-Text、つまり文章入力、文章出力で実行するモデルある。具体的には入力は`タスク：問題`、出力は`回答`という形式で全てのタスクを解く。

<br>

## T5の基本構造

T5の基本構造はTranformerと全く同じでMulti-headのAttention機構を備えたEncoderとDecoderを持つ。この部分はBERTと大きく異なる部分である。


[encdec1]:{{"/enc-dec-image1.png"|prepend:site.imgrepo}}